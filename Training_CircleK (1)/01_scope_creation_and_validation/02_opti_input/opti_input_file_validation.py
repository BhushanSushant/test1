# Databricks notebook source
# MAGIC %md
# MAGIC 
# MAGIC 
# MAGIC 
# MAGIC **Creator: *Milad Eghtedari* **
# MAGIC 
# MAGIC **Date Created: 11/05/2021**
# MAGIC 
# MAGIC **Last Update: 12/08/2021**
# MAGIC 
# MAGIC 
# MAGIC  
# MAGIC  The **Main Tasks** which are being executed in the this Notebook:
# MAGIC * Validate the inputs generated by BUs
# MAGIC * Compare inputs in opti-input file to scope table
# MAGIC 
# MAGIC **Notes**:
# MAGIC * Check header and footer of each tab
# MAGIC * This notebook uses items in scope table (circlek_db.lp_dashboard_items_scope) as reference to compare
# MAGIC 
# MAGIC <!-- 
# MAGIC ** Assumption about input tab**:
# MAGIC 
# MAGIC #Following assumption has been assumed about input
# MAGIC #* New item is added as secondary (not as orphan or primary)
# MAGIC #* There is not any new PF unless existing PF is splited
# MAGIC #* PF should not have multiple CSC
# MAGIC #* Specific categories like beer, soft drink, and packed bev has CSC -->

# COMMAND ----------

### Libraries 

import datetime
from pyspark.sql import *
from pyspark.sql import functions as sf
from pyspark.sql.functions import countDistinct
# from pyspark.sql.types import StringType,BooleanType,DateType, IntegerType
from pyspark.sql.types import *

import re
import pandas as pd
import numpy as np
from datetime import timedelta
import datetime as dt
pd.set_option("display.max_columns", None) # shows all columns in dataframe
pd.set_option('display.max_rows', None) # shows all rows in dataframe

# COMMAND ----------

# MAGIC %md
# MAGIC ##Input creation

# COMMAND ----------

### list of strategies
strategy_list = ["Balanced","Grow Revenue","Maximize Revenue","Grow Margin","Maximize Margin"]

### list of ending number rules
end_num_list = ["No Rule",
"Whole Numbers",
"5",
"9",
"0;5",
"0;9",
"5;9",
"0;5;9",
"0.99",
"0.49;0.99",
"0.29;0.49;0.69;0.99",
"0;0.50;0.75",
"0;0.25;0.50;0.75",
"No Decimals.0-30:all; 30-50:2to9; Over 50:5or9",
"No Decimals.0-50:all; 50-90:2to9; Over 90:5or9",
"Other"
]

### list product gap rules
gap_rule_list = ["Min 10% Lower",
"More Exp",
"Higher Mrgn%",
"Same Retail",
"More Exp;cheap per ML",
"More Exp;cheap per unit",
"More Exp;cheap per oz",
"More Exp;cheap per liter",
"More Exp;lower mrgn%;cheap per unit",
"More Exp;higher mrgn%",
"Other"
]

### list of relationshipe types
gap_type_list = ["Item to Item",
"PF to PF",
"PF to Item",
"Item to PF"]

### list of business unit
Business_Units = ["1400 - Florida Division",
                  "1600 - Coastal Carolina Division",
                  "1700 - Southeast Division",
                  "1800 - Rocky Mountain Division",
                  "1900 - Gulf Coast Division",
                  "2800 - Texas Division",
                  "3100 - Grand Canyon Division",
                  "4200 - Great Lakes Division",
                  "2600 - West Coast Division",
                  "4100 - Midwest Division",
                  "2900 - South Atlantic Division",
                  "4300 - Heartland Division",
                  "QUEBEC OUEST",
                  "Central Division",
                  "Western Division",
                  "QUEBEC EST - ATLANTIQUE",
                  "Norway",
                  "Ireland",
                  "Sweden",
                  "Denmark",
                  "Poland"
                 ]
Business_Units_Store = ["Florida",
                        "Coastal Carolinas",
                        "Southeast",
                        "Rocky Mountain",
                        "Gulf Coast",
                        "Texas",
                        "Grand Canyon",
                        "Great Lakes",
                        "West Coast",
                        "Midwest",
                        "South Atlantic",
                        "Heartland",
                        "Quebec West",
                        "Central Canada",
                        "Western Canada",
                        "Quebec East",
                        "Norway",
                        "Ireland",
                        "Sweden",
                        "Denmark",
                        "Poland",
                       ]

BU_Abbreviation = ["fl","cc","se","rm","gc","tx","gr","gl","wc","mw","sa","hd",  "qw","ce","wd","qe","no","ie","sw","dk","po"]

EU_Country_Code = ["","","","","","","","","","","","","","","","" ,"NO","IE","SE","DK","PL"]

bu_map = pd.DataFrame({"BU" : Business_Units, "BU_store" : Business_Units_Store, "BU_Abb" : BU_Abbreviation, "EU_code":EU_Country_Code})
dbutils.widgets.dropdown("01.Business Unit", "1400 - Florida Division", Business_Units)
dbutils.widgets.text("02.Clusters", "5")
# dbutils.widgets.text("03.Start Date", "2019-10-01")
# dbutils.widgets.text("04.End Date", "2021-09-30")
business_unit = dbutils.widgets.get('01.Business Unit') # business unit in circlek_db.lp_dashboard_items_scope
num_clusters = int(dbutils.widgets.get('02.Clusters'))
# dbutils.widgets.remove("03.Start Date")
# dbutils.widgets.remove("04.End Date")
# start_date = dbutils.widgets.get('03.Start Date')
# end_date = dbutils.widgets.get('04.End Date')
start_date = "2019-12-04"
end_date = "2021-11-30"
BU = bu_map.BU_Abb[bu_map.BU == business_unit]
bu_test_control = list(bu_map.BU_store[bu_map.BU == business_unit])[0] # business unit in circlek_db.grouped_store_list 
BU_CODE = list(bu_map.EU_code[bu_map.BU == business_unit])[0]
NA = ["fl","cc","se","rm","gc","tx","gr","qw","ce","gl","qe","wd","hd","mw","sa","wc"]
EU = ["no","ie","sw","dk","po"]
TRANSACTION_YEARS = [str(x) for x in range(int(re.findall(r"(\d{4})",str(start_date))[0]),int(re.findall(r"(\d{4})",str(end_date))[0])+1)]

# COMMAND ----------

# MAGIC %md
# MAGIC ##User inputs

# COMMAND ----------

#####################################################################
#### ENTER INPUTS HERE
####################################################################

#Enter the number of clusters in the BU in "02.Cluster"
#Choose BU from dropdown "01.Business Unit"
#Choose start date and end date to generate price and cost in "03.Start Date", "04.End Date"


###
### replace this with the location of your scope file on dbfs
###
# input_file =
#input_file = "/dbfs/FileStore/Milad/CC_Opti_Input_Format_test.xlsx"
input_file="/dbfs/test/GL_Opti_Input_Format_20211220_2nd_Round_Rules_Final_prod_gap_rules_fixed.xlsx"
# input_file = "/dbfs/FileStore/Milad/Opti_Input_Format_Sample_v1_9_test_NA.xlsx"          #file received from BU 
# input_file = "/dbfs/FileStore/Milad/Opti_Input_Format_Sample_v1_9_test_EU.xlsx"          #file received from BU 

# COMMAND ----------

#######################################
### reading excel file and scope table
#######################################

item_sheet = "Item Level Inputs"  # item level inputs
strategy_sheet = "Optimization Strategy" # optimization strategy 
csc_sheet = "CSC Level Inputs" # csc level inputs
# csc_past_sheet = "CSC Level Inputs (Past)" # csc level past values
pf_sheet = "PF Level Inputs" # pf level inputs
state_zone_sheet = "State & Zone Level Cats" # categories that optimized in zone or state level
prod_gap_sheet = "Product Gap Rules"            #Product gap riules constraint
# prod_gap_past_sheet = "Product Gap Rules (Past)"            #Product gap rules constraint in past


# items = pd.read_excel(input_file, sheet_name = item_sheet, skiprows = 5)
strategies = pd.read_excel(input_file, sheet_name = strategy_sheet, skiprows = 5, usecols = "A:G")
strategies =strategies.drop([0])
csc = pd.read_excel(input_file, sheet_name = csc_sheet, skiprows = 5, usecols = "A:L")
csc =csc.drop([0])
pf = pd.read_excel(input_file, sheet_name = pf_sheet, skiprows = 5)
pf = pf.drop([0])
states = pd.read_excel(input_file, sheet_name = state_zone_sheet, skiprows = 5, usecols = "A:C")
zones = pd.read_excel(input_file, sheet_name = state_zone_sheet, skiprows = 5, usecols = "E:G")
################################################################################
### check ending row of zones and states 
################################################################################
# zones = zones[:-1] # -1 value is how many rows zones is shorter than states if zones table is smaller than states table
###
### uncomment if states table is smaller than zones table
###
#states = states[:-1] # -1 value is how many rows states is shorter than zones if states table is smaller than zones table
prod_gap = pd.read_excel(input_file, sheet_name = prod_gap_sheet, skiprows = 5, usecols = "A:S")
prod_gap = prod_gap.drop([0])


# items.columns = items.columns.str.replace(' ', '_')
# items.columns = items.columns.str.replace('.', '_')
strategies.columns = strategies.columns.str.replace(' ', '_')
strategies.columns = strategies.columns.str.replace('.', '_')
strategies.columns = strategies.columns.str.replace('*', '')
strategies.columns = strategies.columns.str.replace('(', '')
strategies.columns = strategies.columns.str.replace(')', '')
csc.columns = csc.columns.str.replace(' ', '_')
csc.columns = csc.columns.str.replace('.', '_')
csc.columns = csc.columns.str.replace('(', '')
csc.columns = csc.columns.str.replace(')', '')
csc.columns = csc.columns.str.replace('%', 'p')
csc.columns = csc.columns.str.replace('-', '_')
csc.columns = csc.columns.str.replace('___', '_')
csc.columns = csc.columns.str.replace('__', '_')
# csc_past.columns = csc_past.columns.str.replace(' ', '_')
# csc_past.columns = csc_past.columns.str.replace('-', '_')
# csc_past.columns = csc_past.columns.str.replace('__', '_')
# csc_past.columns = csc_past.columns.str.replace('.', '_')
# csc_past.columns = csc_past.columns.str.replace('(', '')
# csc_past.columns = csc_past.columns.str.replace(')', '')
# csc_past.columns = csc_past.columns.str.replace('%', 'p')
pf.columns = pf.columns.str.replace(' ', '_')
pf.columns = pf.columns.str.replace('.', '_')
pf.columns = pf.columns.str.replace('(', '')
pf.columns = pf.columns.str.replace(')', '')
pf.columns = pf.columns.str.replace('$', 'd')
pf.columns = pf.columns.str.replace('/', '_')
pf.columns = pf.columns.str.replace('___', '_')
pf.columns = pf.columns.str.replace('__', '_')
states.columns = states.columns.str.replace(' ', '_')
states.columns = states.columns.str.replace('.', '_')
states.columns = states.columns.str.replace('(', '')
states.columns = states.columns.str.replace(')', '')
zones.columns = zones.columns.str.replace(' ', '_')
zones.columns = zones.columns.str.replace('.', '_')
zones.columns = zones.columns.str.replace('(', '')
zones.columns = zones.columns.str.replace(')', '')
zones.rename(columns={"BU_1":"BU"},inplace=True)
# prod_gap_past.columns = prod_gap_past.columns.str.replace(' ', '_')
# prod_gap_past.columns = prod_gap_past.columns.str.replace('.', '_')
# prod_gap_past.columns = prod_gap_past.columns.str.replace('/', '_')
prod_gap.columns = prod_gap.columns.str.replace(' ', '_')
prod_gap.columns = prod_gap.columns.str.replace('.', '_')
prod_gap.columns = prod_gap.columns.str.replace('/', '_')
prod_gap.columns = prod_gap.columns.str.replace('__', '_')

# if set(BU).issubset(set(NA)):
#   items.upc = items.upc.astype(str).apply(lambda x: x.replace('.0',''))
#   items.New_upc = items.New_upc.astype(str).apply(lambda x: x.replace('.0',''))
#   items.sell_unit_qty = items.sell_unit_qty.astype(str).apply(lambda x: x.replace('.0',''))
#   items.tempe_product_key = items.tempe_product_key.astype(str).apply(lambda x: x.replace('.0',''))
#   items = items.replace('nan', np.nan)
# else:
#   items.EAN = items.EAN.astype(str).apply(lambda x: x.replace('.0',''))
#   items.New_EAN = items.New_EAN.astype(str).apply(lambda x: x.replace('.0',''))
#   items.JDE_Number = items.JDE_Number.astype(str).apply(lambda x: x.replace('.0',''))
#   items.trn_item_sys_id = items.trn_item_sys_id.astype(str).apply(lambda x: x.replace('.0',''))
#   items = items.replace('nan', np.nan)
csc = csc.replace('nan', np.nan)  
pf.UPC_Item_Number = pf.UPC_Item_Number.astype(str).apply(lambda x: x.replace('.0',''))
pf.Sell_Unit_Qty = pf.Sell_Unit_Qty.astype(str).apply(lambda x: x.replace('.0',''))
pf.Product_Key_Trn_Item_Sys_ID_EAN_No_ = pf.Product_Key_Trn_Item_Sys_ID_EAN_No_.astype(str).apply(lambda x: x.replace('.0',''))
pf = pf.replace('nan', np.nan)
prod_gap.Dep_UPC_Item_Number = prod_gap.Dep_UPC_Item_Number.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap.Dep_Sell_Unit_Qty = prod_gap.Dep_Sell_Unit_Qty.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap.Dep_Product_Key_Sys_ID_EAN_No_ = prod_gap.Dep_Product_Key_Sys_ID_EAN_No_.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap.Ind_UPC_Item_Number = prod_gap.Ind_UPC_Item_Number.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap.Ind_Sell_Unit_Qty = prod_gap.Ind_Sell_Unit_Qty.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap.Ind_Product_Key_Sys_ID_EAN_No_ = prod_gap.Ind_Product_Key_Sys_ID_EAN_No_.astype(str).apply(lambda x: x.replace('.0',''))
prod_gap = prod_gap.replace('nan', np.nan)

#Note for Europe BU
################################################################################################
## column name in BU_initial_scope excel sheet ##  equivalent in scope table                  ##
################################################################################################
## trn_item_sys_id                             ##  product_key                                ##
## EAN                                         ##  UPC                                        ##
## JDE Number                                  ##  item_number                                ##
################################################################################################


scope = spark.sql("""Select BU, category_name, subcategory_name, item_name, upc, product_key ,item_number, modelling_level,
                            scope, price_family, sell_unit_qty, category_special_classification
                              From circlek_db.lp_dashboard_items_scope 
                                Where bu = '{}'
                              """.format(business_unit))

items_scope= scope.toPandas()
# items_scope = pd.read_excel("/dbfs/FileStore/Milad/scope_table_EU.xlsx", skiprows = 5)
# items_scope = pd.read_excel("/dbfs/FileStore/Milad/scope_table_NA.xlsx", skiprows = 5)
items_scope.upc = items_scope.upc.astype(str).apply(lambda x: x.replace('.0',''))
items_scope.sell_unit_qty = items_scope.sell_unit_qty.astype(str).apply(lambda x: x.replace('.0',''))
items_scope.product_key = items_scope.product_key.astype(str).apply(lambda x: x.replace('.0',''))
items_scope.item_number = items_scope.item_number.astype(str).apply(lambda x: x.replace('.0',''))
items_scope = items_scope.replace('nan', np.nan)

### Categories that have CSC
# cat_csc=set(items.category[items.Category_Special_Classification.notna() | items.New_CSC.notna()])
cat_csc=set(items_scope.category_name[(items_scope.category_special_classification!="No_CSC")&(items_scope.scope=="Y")
])
# cat_csc=set(items_scope.category_name[(items_scope.category_special_classification.notna())&(items_scope.category_special_classification!="No_CSC")])

# COMMAND ----------

# DBTITLE 1,Scope table validations
###
### All primary items of price family are in one CSC
###
prim_csc = items_scope.loc[((items_scope.scope=="Y")  &(items_scope.category_special_classification != "No_CSC")&(items_scope.price_family!="No_Family"))]
prim_csc = prim_csc[prim_csc.groupby([ 'price_family'])["category_special_classification"].transform('nunique')>1].sort_values(by=["price_family"])


###
### All primary items of price family are in one subcategory for No_CSC
###

prim_sub = items_scope.loc[(items_scope.scope=="Y") & (items_scope.category_special_classification == "No_CSC")&(items_scope.price_family!="No_Family")]
prim_sub = prim_sub[(prim_sub.groupby([ 'price_family'])["subcategory_name"].transform('nunique')>1)].sort_values(by=["price_family"])

scope_pf = pd.concat([prim_sub,prim_csc]).drop_duplicates() 
print("There were",prim_sub.shape[0],"pf with multiple sub categories in scope table")
print("There were",prim_csc.shape[0],"pf with multiple CSC in scope table")
print("There were total",scope_pf.shape[0],"pf with issues in scope table")
scope_pf.sort_values(by=["price_family"])


# COMMAND ----------

# MAGIC %md
# MAGIC ## Generating Price_cost_Cluster

# COMMAND ----------

#####################################
### price_cost functions for NA
#####################################

###
### Created by Aadarsh ("https://adb-8165306836189773.13.azuredatabricks.net/?o=8165306836189773#notebook/3997342621309796/command/3997342621309797")
###

###  Scope and store list

def get_scope_NA(busines_unit):
  scope = spark.sql("""Select product_key from 
                             (Select product_key, last_update, rank() over(partition by product_key order by last_update desc) as latest 
                                From circlek_db.lp_dashboard_items_scope 
                                Where bu = '{}'
                              ) a
                              Where latest =  1""".format(business_unit))
  product_scope = [x[0] for x in scope.select("product_key").collect()]
  return product_scope

def get_sites_NA(bu_test_control):
  sites = spark.sql("""Select site_id from circlek_db.grouped_store_list 
                              Where business_unit = '{}' 
                              and group = 'Test' """.format(bu_test_control))

  sites_scope = [x[0] for x in sites.select("site_id").collect()]
  return sites_scope


def get_cluster_info_NA(bu_test_control):
  site_info = spark.sql("""Select site_id, Cluster from circlek_db.grouped_store_list 
                              Where business_unit = '{}' 
                              and group = 'Test' """.format(bu_test_control))
  
  return site_info

### PDI connections - latest price as of 10/1

## GETTING THE RIGHT ENVIRONMENT FOR EACH BU: 

def get_env(bu):
  env = "Tempe"
  if bu == "Quebec West":
    env = "Laval"
  elif bu == "Central Canada":
    env = "Toronto"

  return env

## Fetch PDI prices + filter for items x sites from latest txns.
def get_pdi_price(start_date, end_date, env, business_unit, product_scope, sites_scope):
    price_table = spark.sql("""
    SELECT

      all_bu_p.product_key as tempe_product_key,
      all_bu_p.item_number as item_number,
      all_bu_p.category_desc as category_desc,
      all_bu_p.sub_category_desc as sub_category_desc,
      all_bu_p.item_desc as item_desc,
      all_bu_s.division_desc as division_desc,
      all_bu_s.site_number_corporate,
      all_bu_s.site_number,
      vw_irp.gross_price,
      vw_irp.net_price,
      vw_irp.environment_name,
      all_bu_s.site_state_id

    FROM dl_edw_na.vw_item_retail_prices vw_irp 

    JOIN dl_localized_pricing_all_bu.site all_bu_s 
      ON vw_irp.site_number = all_bu_s.site_number 
        AND vw_irp.environment_name = all_bu_s.sys_environment_name
    JOIN dl_localized_pricing_all_bu.product all_bu_p 
      ON vw_irp.item_number = all_bu_p.item_number
        AND vw_irp.environment_name = all_bu_p.environment_name
        AND vw_irp.package_quantity = all_bu_p.sell_unit_qty
    WHERE
        effective_start_date <= '{1}'
        AND effective_end_date >= '{1}'
        AND division_desc = '{2}'

    """.format(start_date, end_date, business_unit))\
    .filter(sf.col('environment_name') == env)

    price_table = price_table.filter(sf.col('tempe_product_key').isin(product_scope)).filter(sf.col('site_number').isin(sites_scope))
    return price_table
  
## Fetch PDI prices + filter for items x sites from scoped lists

def get_pdi_cost(start_date, end_date, env, business_unit, product_scope, sites_scope):
    cost_table = spark.sql("""
    SELECT

      all_bu_p.product_key as tempe_product_key,
      all_bu_p.item_number as item_number,
      all_bu_p.category_desc as category_desc,
      all_bu_p.sub_category_desc as sub_category_desc,
      all_bu_p.item_desc as item_desc,
      all_bu_s.division_desc as division_desc,
      all_bu_s.site_number_corporate,
      all_bu_s.site_number,
      vw_ic.total_cost,
      vw_ic.unit_cost,
      vw_ic.environment_name,
      all_bu_s.site_state_id

    FROM dl_edw_na.vw_item_costs vw_ic 

    JOIN dl_localized_pricing_all_bu.site all_bu_s 
      ON vw_ic.site_number = all_bu_s.site_number 
        AND vw_ic.environment_name = all_bu_s.sys_environment_name
    JOIN dl_localized_pricing_all_bu.product all_bu_p 
      ON vw_ic.item_number = all_bu_p.item_number
        AND vw_ic.environment_name = all_bu_p.environment_name
        AND vw_ic.package_quantity = all_bu_p.sell_unit_qty
    WHERE
        effective_start_date <= '{1}'
        AND effective_end_date >= '{1}'
        AND division_desc = '{2}'
        AND cost_is_promo_flag != 'true'
    """.format(start_date, end_date, business_unit))\
    .filter(sf.col('environment_name') == env)
    
    cost_table = cost_table.filter(sf.col('tempe_product_key').isin(product_scope)).filter(sf.col('site_number').isin(sites_scope))
    return cost_table
  
def get_price_cost_rollup(price_table, cost_table, site_info):

  price_cost = price_table.join(cost_table, ['tempe_product_key', 'site_number'], 'left').select(price_table["*"], "total_cost", "unit_cost")
  cluster_price_cost =  price_cost.join(site_info, [site_info.site_id == price_cost.site_number], 'left')
  cluster_price_cost =  cluster_price_cost.withColumn('Cluster', cluster_price_cost.Cluster.cast(IntegerType()))

  win = Window.partitionBy(['division_desc', 'tempe_product_key', 'Cluster']).orderBy(sf.desc('count'))
  rolled_price = cluster_price_cost.filter(sf.col('gross_price').isNotNull()).\
                                  filter(sf.col('total_cost').isNotNull()).\
                                  groupBy(['division_desc','tempe_product_key', 'Cluster','gross_price']).count().\
                                  withColumn('rank', sf.rank().over(win)).filter(sf.col('rank') == 1).drop('rank').\
                                  groupBy(['division_desc','tempe_product_key']).pivot('Cluster').avg('gross_price')

  rolled_price = rolled_price.select([sf.col(c).alias("retail_price_" + c) for c in rolled_price.columns])
  
  rolled_cost = cluster_price_cost.filter(sf.col('gross_price').isNotNull()).\
                                  filter(sf.col('total_cost').isNotNull()).\
                                  groupBy(['division_desc','tempe_product_key', 'Cluster','total_cost']).count().\
                                  withColumn('rank', sf.rank().over(win)).filter(sf.col('rank') == 1).drop('rank').\
                                  groupBy(['division_desc','tempe_product_key']).pivot('Cluster')\
                                                                 .agg(sf.avg('total_cost'))
  rolled_cost = rolled_cost.select([sf.col(c).alias("total_cost_" + c) for c in rolled_cost.columns])
  rolled_price_cost = rolled_price.join(rolled_cost, [rolled_price.retail_price_division_desc == rolled_cost.total_cost_division_desc, 
                                                      rolled_price.retail_price_tempe_product_key == rolled_cost.total_cost_tempe_product_key]).\
                                  drop('total_cost_division_desc').drop('total_cost_tempe_product_key').\
                                  withColumnRenamed('retail_price_division_desc', 'division_desc').\
                                  withColumnRenamed('retail_price_tempe_product_key', 'tempe_product_key')


  return  rolled_price_cost    

# COMMAND ----------

######################################
### price_cost functions for EU
######################################




#Note
################################################################################################
## column name in initial scope excel sheet ##  equivalent here(or scope table)               ##
################################################################################################
## trn_item_sys_id                          ##  product_key                                   ##
## EAN                                      ##  UPC                                           ##
## JDE Number                               ##  item_number                                   ##
################################################################################################




###
### Created by Darren ("https://adb-8165306836189773.13.azuredatabricks.net/?o=8165306836189773#notebook/3997342621324628/command/3997342621324629")


def pandas_to_spark(pandas_df):
    def equivalent_type(f):
        if f == 'datetime64[ns]': return TimestampType()
        elif f == 'int64': return LongType()
        elif f == 'int32': return IntegerType()
        elif f == 'float64': return FloatType()
        else: return StringType()
    def define_structure(string, format_type):
        try: typo = equivalent_type(format_type)
        except: typo = StringType()
        return StructField(string, typo)

    columns = list(pandas_df.columns)
    types = list(pandas_df.dtypes)
    struct_list = []
    for column, typo in zip(columns, types): 
        struct_list.append(define_structure(column, typo))
    p_schema = StructType(struct_list)
    return sqlContext.createDataFrame(pandas_df, p_schema)
  
###  Scope and store list
def get_scope_EU(business_unit, modelling_level):
  product_identifier = 'product_key' if modelling_level == 'Sys ID' else 'upc'
  col_filter = '!=' if modelling_level == 'Sys ID' else '='
  scope = spark.sql("""SELECT {0} 
                       FROM (SELECT {0}, last_update, 
                                    RANK() OVER (PARTITION BY {0} 
                                                 ORDER BY last_update DESC) AS latest 
                             FROM circlek_db.lp_dashboard_items_scope 
                             WHERE bu = '{1}' AND
                                   modelling_level {2} 'EAN') a
                       WHERE latest = 1""".format(product_identifier,
                                                  business_unit, 
                                                  col_filter))
  #product_scope = [x[0] for x in scope.select("product_key").collect()]
  return scope.select(product_identifier).distinct()

def get_sites_EU(bu_test_control):
  sites = spark.sql("""SELECT site_id AS trn_site_number
                       FROM circlek_db.grouped_store_list 
                       WHERE business_unit = '{}' AND 
                             group = 'Test' """.format(bu_test_control))

  #sites_scope = [x[0] for x in sites.select("site_id").collect()]
  return sites.select('trn_site_number').distinct()

def get_site_cluster_EU(bu_test_control):
  site_info = spark.sql("""SELECT site_id AS trn_site_number, Cluster 
                           FROM circlek_db.grouped_store_list 
                           WHERE business_unit = '{}' AND 
                                 group = 'Test' """.format(bu_test_control)).distinct()
  return site_info

# mode of price, unit cost and vat_rate on latest day
def get_mode_on_latest_transaction_date(measurement):
  assert measurement in {'trn_item_unit_price', 'unit_cost', 'trn_vat_rate'}, 'Wrong measurement column'
  measurement_mode_latest_day = spark.sql("""
  WITH transaction_w_rank AS 
  (SELECT product_key,
          Cluster,
          {0},
          DENSE_RANK() OVER (PARTITION BY Cluster, product_key
                             ORDER BY COUNT(*) DESC) AS {0}_rank,
          ROW_NUMBER() OVER (PARTITION BY Cluster, product_key
                             ORDER BY COUNT(*) DESC) AS {0}_rnum
   FROM product_transaction_data
   GROUP BY product_key, Cluster, {0})
   SELECT product_key,
          Cluster, 
          {0} AS {0}_mode
   FROM transaction_w_rank
   WHERE {0}_rank = 1 AND
         {0}_rnum = 1
  """.format(measurement))
  return measurement_mode_latest_day

# mode of price, unit cost and vat_rate on latest day
def get_ean_mode_on_latest_transaction_date(measurement):
  assert measurement in {'trn_item_unit_price', 'unit_cost', 'trn_vat_rate'}, 'Wrong measurement column'
  measurement_mode_latest_day = spark.sql("""
  WITH transaction_w_rank AS 
  (SELECT upc,
          Cluster,
          {0},
          DENSE_RANK() OVER (PARTITION BY Cluster, upc
                             ORDER BY COUNT(*) DESC) AS {0}_rank,
          ROW_NUMBER() OVER (PARTITION BY Cluster, upc
                             ORDER BY COUNT(*) DESC) AS {0}_rnum
   FROM product_transaction_data_ean
   GROUP BY upc, Cluster, {0})
   SELECT upc,
          Cluster, 
          {0} AS {0}_mode
   FROM transaction_w_rank
   WHERE {0}_rank = 1 AND
         {0}_rnum = 1
  """.format(measurement))
  return measurement_mode_latest_day


def get_price_cost():
  ### Add in Salad and Sandwiches to the scope list which were dropped in May refresh
  sw_scope_jan_refresh = pd.read_excel('/dbfs/SE_Repo/Price_Refresh_JAN2021/Optimization/Inputs/Hierarchy_w_Price_Family.xlsx')
  sw_scope_jan_refresh = sw_scope_jan_refresh.loc[sw_scope_jan_refresh['Category_Name'].isin(['Salads', 'Sandwiches']), ]
  extra_scope_prod_key = sw_scope_jan_refresh.rename(mapper={'Sys_ID': 'product_key'}, axis='columns')['product_key']
  # there are no ean level product in those two categories
  extra_scope_ean_level = sw_scope_jan_refresh.loc[sw_scope_jan_refresh['EAN'] == sw_scope_jan_refresh['Sys_ID_Join'], ].rename(mapper={'EAN': 'upc'}, axis='columns')['upc']
  extra_scope_sys_id_level = sw_scope_jan_refresh.loc[sw_scope_jan_refresh['EAN'] != sw_scope_jan_refresh['Sys_ID_Join'], ].rename(mapper={'Sys_ID': 'product_key'}, axis='columns')['product_key']

  extra_scope_sys_id_level_spark_df = pandas_to_spark(extra_scope_sys_id_level.to_frame())
  extra_scope_sys_id_level_spark_df.createOrReplaceTempView("extra_scope_sys_id_level")

  sw_scope_jan_refresh_spark_df = pandas_to_spark(sw_scope_jan_refresh)
  sw_scope_jan_refresh_spark_df.createOrReplaceTempView("sw_extra_scope_jan_refresh")

  sys_id_level_product_scope = get_scope_EU(business_unit, 'Sys ID')
  sys_id_level_product_scope = sys_id_level_product_scope.unionAll(extra_scope_sys_id_level_spark_df)
  EAN_level_product_scope = get_scope_EU(business_unit, 'EAN')
  # test_sites_to_cluster = get_site_cluster_EU(business_unit)


  sys_id_level_product_scope.createOrReplaceTempView("sys_id_level_product_scope")
  EAN_level_product_scope.createOrReplaceTempView("EAN_level_product_scope")
  # test_sites_to_cluster.createOrReplaceTempView("test_sites_to_cluster")

  # store to cluster mapping alternative while the main test sites table is in defect
  test_sites_to_cluster = pd.read_csv('/dbfs/SE_Repo/Price_Refresh_MAY2021/Modelling_Inputs/store_cluster_map_may_2021.csv')
  test_sites_to_cluster = pandas_to_spark(test_sites_to_cluster).withColumnRenamed('Cluster_Final', 'Cluster')
  test_sites_to_cluster.createOrReplaceTempView("test_sites_to_cluster")

  ###Use the mode unit_price, cost, and vat_rate from transaction data on the last day an item was sold in each test store

  ### sys_id_level modelling
  # join with transaction data, limit to last day when a site * item was sold
  product_transaction_data = spark.sql("""
  SELECT t.trn_transaction_date,
       s.Cluster,
       p.product_key,
       t.trn_item_unit_price,
       t.trn_cost / t.trn_sold_quantity AS unit_cost,
       t.trn_vat_rate,
       DENSE_RANK() OVER (PARTITION BY p.product_key, s.Cluster 
                          ORDER BY t.trn_transaction_date DESC) AS date_order 
  FROM dw_eu_rep_pos.pos_transactions_f t
  INNER JOIN 
  sys_id_level_product_scope p
  ON 
  t.trn_item_sys_id = p.product_key
  INNER JOIN test_sites_to_cluster s
  ON
  t.trn_site_sys_id = s.trn_site_sys_id
  WHERE t.trn_transaction_year IN ({0}) AND
      t.trn_country_cd = '{1}' AND
      t.trn_transaction_date >= '{2}' AND t.trn_transaction_date <= '{3}' AND
      t.trn_item_unit_price > 0 AND
      t.trn_cost > 0
      --t.trn_promotion_id = -1
  ORDER BY p.product_key, s.Cluster, t.trn_transaction_date""".format(', '.join(y for y in TRANSACTION_YEARS),
                                                                    BU_CODE,
                                                                    start_date,
                                                                    end_date)).filter(sf.col('date_order') == 1)

  product_transaction_data.createOrReplaceTempView('product_transaction_data')



  get_mode_on_latest_transaction_date('trn_item_unit_price').createOrReplaceTempView('unit_price_mode_tbl')
  get_mode_on_latest_transaction_date('unit_cost').createOrReplaceTempView('unit_cost_mode_tbl')
  get_mode_on_latest_transaction_date('trn_vat_rate').createOrReplaceTempView('trn_vat_rate_tbl')

  product_mode_cost_price_vat_sys_id = spark.sql("""
  SELECT p.product_key,
         p.Cluster,
         trn_item_unit_price_mode AS unit_price_mode,
         unit_cost_mode,
         trn_vat_rate_mode AS vat_rate_mode
  FROM unit_price_mode_tbl p
       JOIN 
       unit_cost_mode_tbl c
       ON p.product_key = c.product_key AND
          p.Cluster = c.Cluster
       JOIN 
       trn_vat_rate_tbl v
       ON p.product_key = v.product_key AND
          p.Cluster = v.Cluster""")

  product_mode_cost_price_vat_sys_id.createOrReplaceTempView('product_mode_cost_price_vat_sys_id')
  #display(product_mode_cost_price_vat_sys_id)

  ### EAN level modelling
  # join with transaction data, limit to last day when a site * item was sold
  product_transaction_data_ean = spark.sql("""
  SELECT t.trn_transaction_date,
         s.Cluster,
         t.trn_item_sys_id AS product_key,
         p.upc,
         t.trn_item_unit_price,
         t.trn_cost / t.trn_sold_quantity AS unit_cost,
         t.trn_vat_rate,
         DENSE_RANK() OVER (PARTITION BY p.upc, s.Cluster 
                            ORDER BY t.trn_transaction_date DESC) AS date_order 
  FROM dw_eu_rep_pos.pos_transactions_f t
  INNER JOIN 
  EAN_level_product_scope p
  ON 
  REPLACE(LTRIM(REPLACE(t.trn_barcode, '0', ' ')), ' ', '0') = p.upc
  INNER JOIN test_sites_to_cluster s
  ON
  t.trn_site_sys_id = s.trn_site_sys_id
  WHERE t.trn_transaction_year IN ({0}) AND
        t.trn_country_cd = '{1}' AND
        t.trn_transaction_date >= '{2}' AND t.trn_transaction_date <= '{3}' AND
        t.trn_item_unit_price > 0 AND
        t.trn_cost > 0
        --t.trn_promotion_id = -1
  ORDER BY p.upc, s.Cluster, t.trn_transaction_date""".format(', '.join(y for y in TRANSACTION_YEARS),
                                                              BU_CODE,
                                                              start_date,
                                                              end_date)).filter(sf.col('date_order') == 1)

  product_transaction_data_ean.createOrReplaceTempView('product_transaction_data_ean')


  get_ean_mode_on_latest_transaction_date('trn_item_unit_price').createOrReplaceTempView('unit_price_mode_tbl')
  get_ean_mode_on_latest_transaction_date('unit_cost').createOrReplaceTempView('unit_cost_mode_tbl')
  get_ean_mode_on_latest_transaction_date('trn_vat_rate').createOrReplaceTempView('trn_vat_rate_tbl')

  product_mode_cost_price_vat_ean = spark.sql("""
  SELECT p.upc,
         p.Cluster,
         trn_item_unit_price_mode AS unit_price_mode,
         unit_cost_mode,
         trn_vat_rate_mode AS vat_rate_mode
  FROM unit_price_mode_tbl p
       JOIN 
       unit_cost_mode_tbl c
       ON p.upc = c.upc AND
          p.Cluster = c.Cluster
       JOIN 
       trn_vat_rate_tbl v
       ON p.upc = v.upc AND
          p.Cluster = v.Cluster""")

  product_mode_cost_price_vat_ean.createOrReplaceTempView('product_mode_cost_price_vat_ean')

  ### get max date and sum of revenue from transaction data
  max_date_total_revenue_sys_id = spark.sql("""
  SELECT p.product_key,
         MAX(t.trn_transaction_date) AS latest_sales_date,
         SUM(t.trn_gross_amount) AS total_sales_amount
  FROM dw_eu_rep_pos.pos_transactions_f t
  INNER JOIN 
  sys_id_level_product_scope p
  ON 
  t.trn_item_sys_id = p.product_key
  INNER JOIN test_sites_to_cluster s
  ON
  t.trn_site_sys_id = s.trn_site_sys_id
  WHERE t.trn_transaction_year IN ({0}) AND
        t.trn_country_cd = '{1}' AND
        t.trn_transaction_date >= '{2}' AND t.trn_transaction_date <= '{3}' AND
        t.trn_item_unit_price > 0 AND
        t.trn_cost > 0
        --t.trn_promotion_id = -1
  GROUP BY p.product_key""".format(', '.join(y for y in TRANSACTION_YEARS),
                           BU_CODE,
                           start_date,
                           end_date))

  max_date_total_revenue_ean = spark.sql("""
  SELECT p.upc,
         MAX(t.trn_transaction_date) AS latest_sales_date,
         SUM(t.trn_gross_amount) AS total_sales_amount
  FROM dw_eu_rep_pos.pos_transactions_f t
  INNER JOIN 
  EAN_level_product_scope p
  ON 
  REPLACE(LTRIM(REPLACE(t.trn_barcode, '0', ' ')), ' ', '0') = p.upc
  INNER JOIN test_sites_to_cluster s
  ON
  t.trn_site_sys_id = s.trn_site_sys_id
  WHERE t.trn_transaction_year IN ({0}) AND
        t.trn_country_cd = '{1}' AND
        t.trn_transaction_date >= '{2}' AND t.trn_transaction_date <= '{3}' AND
        t.trn_item_unit_price > 0 AND
        t.trn_cost > 0
        --t.trn_promotion_id = -1
  GROUP BY p.upc""".format(', '.join(y for y in TRANSACTION_YEARS),
                           BU_CODE,
                           start_date,
                           end_date))

  max_date_total_revenue_sys_id.createOrReplaceTempView('max_date_total_revenue_sys_id')
  max_date_total_revenue_ean.createOrReplaceTempView('max_date_total_revenue_ean')

  ### get product hierarchy from item table
  prod_hier = spark.sql("""
  WITH scope AS
  ((SELECT DISTINCT product_key 
  FROM
  (SELECT product_key,
          RANK() OVER (PARTITION BY product_key ORDER BY last_update DESC) AS latest
   FROM circlek_db.lp_dashboard_items_scope
   WHERE bu = 'Sweden')
  WHERE latest = 1)

  UNION

  (SELECT * FROM extra_scope_sys_id_level))

  SELECT s.product_key, i.item_number, i.item_name, i.item_subcategory, i.item_category
  FROM dw_eu_common.item_d i
  INNER JOIN 
  scope s
  ON 
  i.sys_id = s.product_key""")


  prod_hier.createOrReplaceTempView('prod_hier')
  # display(prod_hier)

  ### get CSC from opti master input
  opti_input = '/dbfs/SE_Repo/Optimization/Inputs/Hierarchy_map.xlsx'
  csc = spark.createDataFrame(pd.read_excel(opti_input)[['Sys ID Join', 'Category Special Classification']].dropna()).distinct()
  #            .withColumnRenamed('Sys ID Join', 'product_key')
  csc_extra = sw_scope_jan_refresh_spark_df.select(['Sys_ID_Join', 'Category_Special_Classification'])
  csc = csc.unionAll(csc_extra)
  csc.createOrReplaceTempView('csc')
  # display(csc)

  ### create output
  # skeleton
  output_skl = spark.sql("""
  WITH scope AS
       (SELECT product_key, upc, price_family, Scope, modelling_level
        FROM (SELECT product_key, upc, price_family, Scope, modelling_level, 
                     RANK() OVER (PARTITION BY product_key ORDER BY last_update DESC) AS latest
              FROM circlek_db.lp_dashboard_items_scope
              WHERE bu = 'Sweden' AND
                    modelling_level != 'EAN') sys_id_level
        WHERE latest = 1 
        UNION 
        SELECT product_key, upc, price_family, Scope, modelling_level
        FROM (SELECT product_key, upc, price_family, Scope, modelling_level, 
                     RANK() OVER (PARTITION BY upc ORDER BY last_update DESC) AS latest
              FROM circlek_db.lp_dashboard_items_scope
              WHERE bu = 'Sweden' AND
                    modelling_level = 'EAN') EAN_level
        WHERE latest = 1
        UNION
        SELECT Sys_ID AS product_key, EAN AS upc, Price_Family AS price_family, Scope, 'Sys ID' AS modelling_level
        FROM sw_extra_scope_jan_refresh)
  SELECT s.*, c.* FROM 
  scope s
  CROSS JOIN
  (SELECT DISTINCT Cluster
  FROM test_sites_to_cluster) c
  /*(SELECT DISTINCT Cluster
  FROM circlek_db.grouped_store_list 
  WHERE business_unit = 'Sweden' AND 
        group = 'Test') c*/
  ORDER BY product_key, Cluster
  """)

  output_skl.createOrReplaceTempView('output_skl')

  # join price cost vat to skeleton
  output_measure = spark.sql("""
  SELECT o.*,
         COALESCE(s.unit_price_mode, e.unit_price_mode) AS unit_price_mode,
         COALESCE(s.unit_cost_mode, e.unit_cost_mode) AS unit_cost_mode,
         COALESCE(s.vat_rate_mode, e.vat_rate_mode) AS vat_rate_mode
  FROM output_skl o
  LEFT JOIN
  product_mode_cost_price_vat_sys_id s
  ON o.product_key = s.product_key AND
     o.Cluster = s.Cluster
  LEFT JOIN
  product_mode_cost_price_vat_ean e
  ON o.upc = e.upc AND
     o.Cluster = e.Cluster""")

  output_measure = output_measure.withColumn('Cluster', sf.col('Cluster').cast('integer'))

  ### pivot output
  output_pivot = output_measure.groupBy(['product_key', 'upc', 'Scope', 'price_family', 'modelling_level'])\
                               .pivot('Cluster').agg(sf.first('unit_price_mode').alias('retail_price'),
                                                     sf.first('unit_cost_mode').alias('total_cost'))
                                                      
                                                     #,sf.first('vat_rate_mode').alias('vat_rate'))

  output_pivot = output_pivot.select([sf.col(c).name(c[2:] +"_"+ c[0]) if c[:1].isdigit()
                                                              else sf.col(c) for c in output_pivot.columns])\
                             .orderBy(output_pivot.product_key.asc())

  output_pivot.createOrReplaceTempView('output_pivot')

  ### Add max_date, total rev, prod hier and CSC to output
  output = spark.sql("""
  SELECT o.*,
         ph.item_number, ph.item_name, ph.item_subcategory, ph.item_category, csc.`Category Special Classification`,
         COALESCE(ms.latest_sales_date, me.latest_sales_date) AS latest_sales_date,
         COALESCE(ms.total_sales_amount, me.total_sales_amount) AS total_sales_amount
  FROM
  output_pivot o
  LEFT JOIN 
  max_date_total_revenue_sys_id ms
  ON o.product_key = ms.product_key
  LEFT JOIN 
  max_date_total_revenue_ean me
  ON o.upc = me.upc
  LEFT JOIN 
  prod_hier ph
  ON o.product_key = ph.product_key
  LEFT JOIN 
  csc 
  ON (o.modelling_level = 'EAN' AND o.upc = csc.`Sys ID Join`) OR o.product_key = csc.`Sys ID Join`
  """)
  return output_pivot



# COMMAND ----------

if set(BU).issubset(set(NA)):
  output = get_price_cost_rollup(get_pdi_price(start_date, end_date, get_env(bu_test_control), business_unit, get_scope_NA(business_unit), get_sites_NA(bu_test_control)), get_pdi_cost(start_date, end_date, get_env(bu_test_control), business_unit, get_scope_NA(business_unit), get_sites_NA(bu_test_control)), get_cluster_info_NA(bu_test_control))
  price_cost = output.toPandas()
#   price_cost.rename(columns={"tempe_product_key": "product_key"})
  price_cost.tempe_product_key = price_cost.tempe_product_key.astype(str).apply(lambda x: x.replace('.0',''))
else:
  output = get_price_cost()
  price_cost = output.toPandas()
  price_cost = price_cost.drop(columns=['Scope','price_family','modelling_level'])
  price_cost = price_cost.reindex(sorted(price_cost.columns), axis=1)
  price_cost = price_cost.astype("float64")
  price_cost.product_key = price_cost.product_key.astype(str).apply(lambda x: x.replace('.0',''))
  price_cost.upc = price_cost.upc.astype(str).apply(lambda x: x.replace('.0',''))
  #price_cost = price_cost.rename(columns={"product_key": "trn_item_sys_id"})
  


# COMMAND ----------

price_cost.head(5)

# COMMAND ----------

# MAGIC %md
# MAGIC ##Optimization Strategy tab validations

# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### checking for missed required fields
###
missed_strategies_1 = strategies[(strategies.BU.isna()) |	(strategies.Category.isna()) |	(strategies.SubCategory.isna()) |	((strategies.Optimization_Strategy_Last_Refresh.isna()) & (strategies.Optimization_Strategy_New.isna()))]

###
### missed CSC for categories with CSC
###
missed_strategies_2 = strategies[(strategies.Category.isin(cat_csc)) & (strategies.Category_Special_Classification_latest_values.isna())]

missed_strategies = pd.concat([missed_strategies_1,missed_strategies_2]).drop_duplicates() 
print("There were",missed_strategies_2.shape[0],"CSC level categories with empty CSC cell")
print("There were total",missed_strategies.shape[0],"rows with missed cell")
missed_strategies


# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### checking for duplicate rows
###
strategies_dup = strategies.loc[strategies.duplicated(["BU","Category"	,"SubCategory","Category_Special_Classification_latest_values"],keep=False)]
print("There were",strategies_dup.shape[0],"duplicate rows")
strategies_dup

# COMMAND ----------

# DBTITLE 1,Checking unique strategy
###
### checking for one optimization strategy for each category*subcategory*csc
###

###
### csc categories have one optimization strategy
###
# strategies_csc_sub_old = strategies[(strategies.Category_Special_Classification_latest_values!="No_CSC")]
# strategies_csc_sub_old = strategies_csc_old [(strategies_csc_old.groupby(['BU', 'Category', 'SubCategory', 'Category_Special_Classification_latest_values'])['Optimization_Strategy_Last_Refresh'].transform('nunique')>1)]
# strategies_csc_sub_new = strategies[(strategies.Category_Special_Classification_latest_values!="No_CSC")]
# strategies_csc_sub_new = strategies_csc_new[ (strategies_csc_new.groupby(['BU', 'Category', 'SubCategory', 'Category_Special_Classification_latest_values'])['Optimization_Strategy_New'].transform('nunique')>1)]

strategies_csc_cat_old =  strategies[(strategies.Category_Special_Classification_latest_values!="No_CSC")]
strategies_csc_cat_old = strategies_csc_cat_old[(strategies_csc_cat_old.groupby(['Category', 'Category_Special_Classification_latest_values'])["Optimization_Strategy_Last_Refresh"].transform('nunique')>1)]

strategies_csc_cat_new =  strategies[(strategies.Category_Special_Classification_latest_values!="No_CSC")]
strategies_csc_cat_new = strategies_csc_cat_new[strategies_csc_cat_new.groupby(['Category', 'Category_Special_Classification_latest_values'])["Optimization_Strategy_New"].transform('nunique')>1]


###
### non csc categories have one optimization
###
strategies_non_csc_old = strategies[(strategies.Category_Special_Classification_latest_values == "No_CSC")]
strategies_non_csc_old = strategies_non_csc_old[(strategies_non_csc_old.groupby(['BU', 'Category', 'SubCategory'])['Optimization_Strategy_Last_Refresh'].transform('nunique')>1)]
strategies_non_csc_new = strategies[(strategies.Category_Special_Classification_latest_values == "No_CSC")]
strategies_non_csc_new = strategies_non_csc_new[ (strategies_non_csc_new.groupby(['BU', 'Category', 'SubCategory'])['Optimization_Strategy_New'].transform('nunique')>1)]


# strategy_unique = pd.concat([strategies_csc_sub_old,strategies_csc_sub_new,strategies_non_csc_old,strategies_non_csc_new,strategies_csc_cat_new,strategies_csc_cat_old]).drop_duplicates()
strategy_unique = pd.concat([strategies_non_csc_new,strategies_csc_cat_new]).drop_duplicates()


print("There were", strategies_non_csc_new.shape[0],"No_CSC category*subcategory combinations with  more than one strategy")
print("There were", strategies_csc_cat_new.shape[0],"category*CSC combinations with more than one strategy")
print("There were total", strategy_unique.shape[0], "combinations with more than one strategy")
strategy_unique.sort_values(by=['Category_Special_Classification_latest_values'])

# COMMAND ----------

# DBTITLE 1,Checking strategy input
###
### strategies should be from standardize list
###
strategies_opt = strategies[((strategies.Optimization_Strategy_Last_Refresh.notna()) & (~strategies.Optimization_Strategy_Last_Refresh.isin(strategy_list))) |((strategies.Optimization_Strategy_New.notna()) & (~strategies.Optimization_Strategy_New.isin(strategy_list)))]
print("There were", strategies_opt.shape[0],"rows with invalid optimization strategy")
strategies_opt

# COMMAND ----------

# DBTITLE 1,Checking for combinations without strategy
###
### combinations of category, subcategory, CSC from scope table that strategy has not been assigned to them
###
items_scope["concat"] = items_scope[["category_name","subcategory_name","category_special_classification"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
items_scope_primary = items_scope[items_scope.scope=="Y"]
strategies['concat'] = strategies[["Category","SubCategory","Category_Special_Classification_latest_values"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
strategies_comb_missed = items_scope_primary[~items_scope_primary.concat.isin(set(strategies.concat))][["category_name","subcategory_name","category_special_classification"]].drop_duplicates()
items_scope = items_scope.drop(columns=['concat'])
strategies = strategies.drop(columns=["concat"])
print("There were",strategies_comb_missed.shape[0],"combinations without strategies")
strategies_comb_missed

# COMMAND ----------

# DBTITLE 1,Checking undefined combinations
###
### combinations of category , subcategory, CSC that are not defined in scope table
###
items_scope["concat"] = items_scope[["category_name","subcategory_name","category_special_classification"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
strategies['concat'] = strategies[["Category","SubCategory","Category_Special_Classification_latest_values"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
strategies_comb_undefined = strategies[~strategies.concat.isin(set(items_scope.concat))].drop_duplicates()
items_scope = items_scope.drop(columns=['concat'])
strategies = strategies.drop(columns=["concat"])
strategies_comb_undefined = strategies_comb_undefined.drop(columns=["concat"])
print("There were", strategies_comb_undefined.shape[0],"combinations that were not defined in scope table")
strategies_comb_undefined

# COMMAND ----------

# MAGIC %md
# MAGIC ##CSC Level Inputs tab validations

# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### Checking for missed required fields
###

missed_csc_1 = csc[(csc.BU.isna()) | (csc.Category.isna()) | (csc.Subcategory.isna()) | (csc.Ending_Number_Rule.isna()) | ((csc.Ending_Number_Rule=="Other") & (csc.Ending_Number_Rule_Other.isna()))] 

###
### missed CSC for categories with CSC
###
missed_csc_2 = csc[(csc.Category.isin(cat_csc)) & (csc.Category_Special_Classification_latest_values.isna())]

missed_csc=pd.concat([missed_csc_1,missed_csc_2]).drop_duplicates()
print("There were",missed_csc_2.shape[0]," csc level categories with empty csc cell")
print("There were total",missed_csc.shape[0],"rows with missed cell")
missed_csc


# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### Checking for duplicate rows
###
csc_dup = csc.loc[csc.duplicated(["BU","Category","Subcategory","Category_Special_Classification_latest_values"],keep=False)]
print("There were",csc_dup.shape[0],"duplicate rows")
csc_dup

# COMMAND ----------

# DBTITLE 1,Checking invalid input
###
### checks for small value
###
# values less than 1% are considered as small change
csc_value_1 = csc[(csc.Max_Price_Increase_p <= 0.01) | (csc.Max_Price_Decrease_p >= -0.01) | (csc.Item_Minimum_Margin_Rate_p <= 0.01) | (csc.Average_Category_Subset_Minimum_Margin_Rate_p <= 0.01) | (csc.Max_drop_in_Qty_allowed_for_an_item_p >= -0.01) ]

###
### checks for large value
###
# values larger than 40% are considered as large change
csc_value_2 = csc[(csc.Max_Price_Increase_p > 0.4) | (csc.Max_Price_Decrease_p < -0.4) | (csc.Item_Minimum_Margin_Rate_p > 0.4) | (csc.Average_Category_Subset_Minimum_Margin_Rate_p > 0.4) | (csc.Max_drop_in_Qty_allowed_for_an_item_p < -0.4) ]

###
### checks for ending number rules
###
csc.Ending_Number_Rule=csc.Ending_Number_Rule.astype(str)
csc_value_3 = csc[~csc.Ending_Number_Rule.isin(end_num_list)]

###
### inconsistant other ending number rule and comment 
###
csc_value_4 = csc[((csc.Ending_Number_Rule=="Other") & (csc.Ending_Number_Rule_Other.isna())) | ((csc.Ending_Number_Rule!="Other") & (csc.Ending_Number_Rule_Other.notna())) ]

###
### checks CSC with different inputs
###
csc["concat"] = csc[['Max_Price_Decrease_p',
       'Max_Price_Increase_p', 'Item_Minimum_Margin_Rate_p',
       'Average_Category_Subset_Minimum_Margin_Rate_p',
       'Max_drop_in_Qty_allowed_for_an_item_p', 'Ending_Number_Rule',
       'Ending_Number_Rule_Other']].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  
csc_value_5 =  csc[(csc.Category_Special_Classification_latest_values!="No_CSC")]
csc_value_5 = csc_value_5[csc_value_5.groupby(['Category', 'Category_Special_Classification_latest_values'])["concat"].transform('nunique')>1].sort_values(by=["Category_Special_Classification_latest_values"])
csc_value_5 = csc_value_5.drop(columns=['concat'])

###
### checks subcategories with NO_CSC and different inputs 
###
csc_value_6 = csc[(csc.Category_Special_Classification_latest_values == "No_CSC")]
csc_value_6 = csc_value_6[ (csc_value_6.groupby(['Category', 'Subcategory'])['concat'].transform('nunique')>1)].sort_values(by=["Subcategory"])
csc_value_6 = csc_value_6.drop(columns=['concat'])


csc = csc.drop(columns=['concat'])
csc_value = pd.concat([csc_value_1,csc_value_2,csc_value_3,csc_value_4,csc_value_5,csc_value_6]).drop_duplicates()
print("There were",csc_value_1.shape[0],"rows with wrong sign or small value")
print("There were",csc_value_2.shape[0],"rows with large value")
print("There were",csc_value_3.shape[0],"rows with invalid ending number rule")
print("There were",csc_value_4.shape[0],"inconsistant Other ending number rule")
print("There were",csc_value_5.shape[0],"CSC with different inputs")
print("There were",csc_value_6.shape[0],"No_CSC subcategories with differernt inputs")
print("There were total", csc_value.shape[0],"rows with invalid inputs")
csc_value.sort_values(by=['Category_Special_Classification_latest_values'])

# COMMAND ----------

# DBTITLE 1,Checking combinations without bounds
###
### combinations of category ,subcategory, CSC from scope table that bounds have not been assigned to them
###
items_scope["concat"] = items_scope[["category_name","subcategory_name","category_special_classification"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
items_scope_primary = items_scope[items_scope.scope=="Y"]
csc['concat'] = csc[["Category","Subcategory","Category_Special_Classification_latest_values"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
csc_comb_missed = items_scope_primary[~items_scope_primary.concat.isin(set(csc.concat))][["category_name","subcategory_name","category_special_classification"]].drop_duplicates()
items_scope = items_scope.drop(columns=['concat'])
csc = csc.drop(columns=["concat"])
print("There were",csc_comb_missed.shape[0],"combinations without Bounds")
csc_comb_missed

# COMMAND ----------

# DBTITLE 1,Checking undefined combinations
###
### combinations of category , subcategory, CSC that are not defined in scope table
###
items_scope["concat"] = items_scope[["category_name","subcategory_name","category_special_classification"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
csc['concat'] = csc[["Category","Subcategory","Category_Special_Classification_latest_values"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
csc_comb_undefined = csc[~csc.concat.isin(set(items_scope.concat))].drop_duplicates()
items_scope = items_scope.drop(columns=['concat'])
csc = csc.drop(columns=["concat"])
csc_comb_undefined = csc_comb_undefined.drop(columns=["concat"])
print("There were",csc_comb_undefined.shape[0],"combinations that were not defined in scope table")
csc_comb_undefined

# COMMAND ----------

# MAGIC %md
# MAGIC ##PF Level Inputs tab validations

# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### checking missed required fields
###
missed_pf_1 = pf[(pf.BU.isna()) |	(pf.Category.isna()) |	(pf.SubCategory.isna()) | (pf.Price_Family.isna())]

###
### missed CSC for categories with CSC
###
missed_pf_2 = pf[(pf.Category.isin(cat_csc)) & (pf.Category_Special_Classification_latest_values.isna())]

###
### orphan items without item's information
###
if set(BU).issubset(set(NA)):
  missed_pf_3 = pf[((pf.Price_Family == "No_Family")&((pf.Item_Name.isna()) | (pf.UPC_Item_Number.isna()) | (pf.Sell_Unit_Qty.isna()) | (pf.Product_Key_Trn_Item_Sys_ID_EAN_No_.isna())))]
else:
  missed_pf_3 = pf[((pf.Price_Family == "No_Family")&((pf.Item_Name.isna()) | (pf.UPC_Item_Number.isna()) | (pf.EAN_Flag.isna()) | (pf.Product_Key_Trn_Item_Sys_ID_EAN_No_.isna())))]
  

missed_pf = pd.concat([missed_pf_1,missed_pf_2,missed_pf_3]).drop_duplicates()
print("There were",missed_pf_2.shape[0],"CSC level categories with empty CSC cell")
print("There were",missed_pf_3.shape[0],"orphan items without item's information")
print("There were total",missed_pf.shape[0],"rows with missed cell")
missed_pf

# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### checking for duplicate rows
###
pf_dup = pf.loc[pf.duplicated(["BU","Category"	,"SubCategory","Category_Special_Classification_latest_values","Price_Family","Product_Key_Trn_Item_Sys_ID_EAN_No_","Item_Name","EAN_Flag","Sell_Unit_Qty","UPC_Item_Number"],keep=False)]
print("There were",pf_dup.shape[0],"duplicate rows")
pf_dup

# COMMAND ----------

# DBTITLE 1,Checking PF CSC validations
###
### Checking all items of price family are in one CSC
###
pf_csc = pf.loc[((pf.Category_Special_Classification_latest_values != "No_CSC")&(pf.Price_Family!="No_Family"))]
pf_csc = pf_csc[pf_csc.groupby([ 'Price_Family'])["Category_Special_Classification_latest_values"].transform('nunique')>1].sort_values(by=["Price_Family"])


###
### Checking all items of price family are in one subcategory for No_CSC
###

pf_sub = pf.loc[(pf.Category_Special_Classification_latest_values == "No_CSC")&(pf.Price_Family!="No_Family")]
pf_sub = pf_sub[(pf_sub.groupby([ 'Price_Family'])["SubCategory"].transform('nunique')>1)].sort_values(by=["Price_Family"])

pf_issue = pd.concat([pf_sub,pf_csc]).drop_duplicates() 
print("There were",pf_sub.shape[0],"pf with multiple sub categories")
print("There were",pf_csc.shape[0],"pf with multiple CSC")
print("There were total",pf_issue.shape[0],"pf with issues")
pf_issue.sort_values(by=["Price_Family"])

# COMMAND ----------

# DBTITLE 1,checking invalid inputs
###
### ceiling price should be higher than floor price
###
pf_value_1 = pf[pf.Price_Floor_Promo_Price_Floor_d > pf.Price_Ceiling_d]

###
### check for negative values
###
pf_value_2 = pf[(pf.Price_Floor_Promo_Price_Floor_d < 0) | (pf.Price_Ceiling_d < 0)]

###
### value outsides 50% range of average price (ceiling %50 below average price, floor %50 above average price)
###
pd.options.mode.chained_assignment = None 
retail_price = items_scope.copy()
if  set(BU).issubset(set(NA)):
  price_cost = price_cost.rename(columns={"tempe_product_key": "product_key"})
  retail_price = pd.merge(retail_price,price_cost, how='left',on=['product_key'])
else:
  retail_price = pd.merge(retail_price,price_cost,how='left',on=["product_key","upc"])
  
if num_clusters == 4:
  retail_price["avg"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4"]].mean(axis=1)
  retail_price["minimum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4"]].min(axis=1)
  retail_price["maximum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4"]].max(axis=1)
elif num_clusters == 5:
  retail_price["avg"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5"]].mean(axis=1)
  retail_price["minimum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5"]].min(axis=1)
  retail_price["maximum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5"]].max(axis=1)
elif num_clusters == 6 :
  retail_price["avg"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5","retail_price_6"]].mean(axis=1)
  retail_price["minimum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5","retail_price_6"]].min(axis=1) 
  retail_price["maximum"] = retail_price[["retail_price_1","retail_price_2","retail_price_3","retail_price_4","retail_price_5","retail_price_6"]].max(axis=1) 

retail_price["concat_1"] = retail_price[["category_name","subcategory_name","category_special_classification","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
###
### check orphan item outsides range of average
###
if set(BU).issubset(set(NA)):
  retail_item = retail_price[(retail_price.price_family == "No_Family")]
  retail_item["concat"] = retail_item[["concat_1","item_name","upc","sell_unit_qty","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item = pf[(pf.Price_Family == "No_Family")]
  pf_item["concat_1"] = pf_item[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item["concat"] = pf_item[["concat_1","Item_Name","UPC_Item_Number","Sell_Unit_Qty","Product_Key_Trn_Item_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item_avg = pf_item.merge(retail_item[["concat","avg"]],how='left')
  pf_value_3_avg = pf_item_avg[(pf_item_avg.Price_Ceiling_d < 0.5 * pf_item_avg.avg) | (pf_item_avg.Price_Floor_Promo_Price_Floor_d > 1.5 * pf_item_avg.avg)].drop(columns=['concat','concat_1','avg'])
else:
  retail_item = retail_price[(retail_price.price_family == "No_Family")]
  retail_item["concat"] = np.where((retail_item.modelling_level
=="EAN"), retail_item[["concat_1","item_name","item_number","upc"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1), retail_item[["concat_1","item_name","item_number","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1))
  pf_item = pf[(pf.Price_Family == "No_Family")]
  pf_item["concat_1"] = pf_item[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item["concat"] = pf_item[["concat_1","Item_Name","UPC_Item_Number","Product_Key_Trn_Item_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item_avg = pf_item.merge(retail_item[["concat","avg"]],how='left')
  pf_value_3_avg = pf_item_avg[(pf_item_avg.Price_Ceiling_d < 0.5 * pf_item_avg.avg) | (pf_item_avg.Price_Floor_Promo_Price_Floor_d > 1.5 * pf_item_avg.avg)].drop(columns=['concat','concat_1','avg'])

###
### check pf outsides range of average
###
retail_cat_avg = retail_price[(retail_price.price_family != "No_Family")]
retail_cat_avg['concat_avg'] = retail_cat_avg['avg'].groupby(retail_cat_avg['concat_1']).transform('mean')
pf_cat = pf[(pf.Price_Family != "No_Family")]
pf_cat["concat_1"] = pf_cat[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
pf_cat_avg = pf_cat.merge(retail_cat_avg[["concat_1","concat_avg"]],how='left').drop_duplicates()
pf_value_4_avg = pf_cat_avg[(pf_cat_avg.Price_Ceiling_d < 0.5 * pf_cat_avg.concat_avg) | (pf_cat_avg.Price_Floor_Promo_Price_Floor_d > 1.5 * pf_cat_avg.concat_avg)].drop(columns=['concat_1','concat_avg'])

###
### value outsides 50% range of cluster price (ceiling %50 below cluster price, floor %50 above cluster price)
###

###
### check orphan item outsides range of cluster 
###
pf_item_cluster = pf_item.merge(retail_item[["concat","minimum","maximum"]],how='left')
pf_value_3_cluster = pf_item_cluster[(pf_item_cluster.Price_Ceiling_d < 0.5 * pf_item_cluster.minimum) | (pf_item_cluster.Price_Floor_Promo_Price_Floor_d > 1.5 * pf_item_cluster.maximum)].drop(columns=['concat','concat_1','minimum',"maximum"])
  
###
### check primary items of pf outsides range of cluster
###
retail_cat_cluster = retail_price[(retail_price.scope=="Y") & (retail_price.price_family != "No_Family") ]
pf_cat = pf[(pf.Price_Family != "No_Family")]
pf_cat["concat_1"] = pf_cat[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
pf_cat_cluster = pf_cat.merge(retail_cat_cluster[["concat_1","minimum",'maximum']],how='left').drop_duplicates()
pf_value_4_cluster = pf_cat_cluster[(pf_cat_cluster.Price_Ceiling_d < 0.5 * pf_cat_cluster.minimum) | (pf_cat_cluster.Price_Floor_Promo_Price_Floor_d > 1.5 * pf_cat_cluster.maximum)].drop(columns=['concat_1','minimum',"maximum"])

###
### checks pf with different inputs
###
pf["concat"] = pf[[ 'Price_Ceiling_d','Price_Floor_Promo_Price_Floor_d']].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  
pf_value_5 =  pf[(pf.Price_Family!="No_Family")]
pf_value_5 = pf_value_5[pf_value_5.groupby(['Category', 'Price_Family'])["concat"].transform('nunique')>1].sort_values(by=["Price_Family"])
pf_value_5 = pf_value_5.drop(columns=['concat'])

pf = pf.drop(columns=['concat'])
pf_value = pd.concat([pf_value_1,pf_value_2,pf_value_3_avg,pf_value_4_avg,pf_value_3_cluster,pf_value_4_cluster,pf_value_5]).drop_duplicates()
print("There were",pf_value_1.shape[0],"rows with price floor higher than price ceiling")
print("There were",pf_value_2.shape[0],"rows with nigative value")
print("There were",pf_value_3_avg.shape[0],"orphan item outside 50% range of average price")
print("There were",pf_value_4_avg.shape[0],"pf outsides 50% range of average price")
print("There were",pf_value_3_cluster.shape[0],"orphan item outsides 50% range of cluster price ")
print("There were",pf_value_4_cluster.shape[0],"pf outsides 50% range of cluster price ")
print("There were",pf_value_5.shape[0],"pf with different inputs")
print("There were total",pf_value.shape[0],"rows with invalid inputs")
pf_value.sort_values(by=['Price_Family'])

# COMMAND ----------

# DBTITLE 1,Checking undefined combinations
###
### combinations of category , subcategory, CSC, PF that are not defined in scope table
###
items_pf = items_scope[(items_scope.price_family != "No_Family")]
items_pf["concat"] = items_pf[["category_name","subcategory_name","category_special_classification","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)

pf_pf = pf[(pf.Price_Family != "No_Family")]
pf_pf["concat"] = pf_pf[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
pf_pf_undefined = pf_pf[~pf_pf.concat.isin(set(items_pf.concat))].drop_duplicates()

###
### combinations of category, subcategory, CSC, PF, item_name, UPC, sell_unit_qty, TPK that are not defined in scope table
###
if set(BU).issubset(set(NA)):
  items_item = items_scope[(items_scope.price_family == "No_Family") ]
  items_item["concat_1"] = items_item[["category_name","subcategory_name","category_special_classification","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  items_item["concat"] = items_item[["concat_1","item_name","upc","sell_unit_qty","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)

  pf_item = pf[(pf.Price_Family == "No_Family")]
  pf_item["concat_1"] = pf_item[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item["concat"] = pf_item[["concat_1","Item_Name","UPC_Item_Number","Sell_Unit_Qty","Product_Key_Trn_Item_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item_undefined = pf_item[~pf_item.concat.isin(set(items_item.concat))].drop_duplicates()
else:
  items_item = items_scope[(items_scope.price_family == "No_Family") ]
  items_item["concat_1"] = items_item[["category_name","subcategory_name","category_special_classification","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  items_item["concat"] = np.where((items_item.modelling_level
=="EAN"), items_item[["concat_1","item_name","item_number","upc"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1), items_item[["concat_1","item_name","item_number","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1))
    
  pf_item = pf[(pf.Price_Family == "No_Family")]
  pf_item["concat_1"] = pf_item[["Category","SubCategory","Category_Special_Classification_latest_values","Price_Family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item["concat"] = pf_item[["concat_1","Item_Name","UPC_Item_Number","Product_Key_Trn_Item_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  pf_item_undefined = pf_item[~pf_item.concat.isin(set(items_item.concat))].drop_duplicates()

pf_undefined = pd.concat([pf_item_undefined,pf_pf_undefined]).drop_duplicates()
pf_undefined = pf_undefined.drop(columns=["concat","concat_1"])
print("There were", pf_pf_undefined.shape[0],"PF combinations that are not defined in scope table")
print("There were", pf_item_undefined.shape[0],"item combinations that are not defined in scope table")
print("There were", pf_undefined.shape[0],"combinations that are not defined in scope table")
pf_undefined

# COMMAND ----------

# MAGIC %md
# MAGIC ##State & Zone Level Cats tab validations

# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### checking missed required fields
###
missed_states = states[states.BU.isna() | states.Categories_Optimized_at_State_Level_Last_Refresh.isna()]
print("There were",missed_states.shape[0],"rows with missed cell")
missed_states


# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### checking missed required fields
###
missed_zones = zones[zones.BU.isna() | zones.Categories_Optimized_at_Zone_Level_Last_Refresh.isna()]
print("There were",missed_zones.shape[0],"rows with missed cell")
missed_zones

# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### checking for duplicate rows
###
states_dup = states.loc[states.duplicated(keep=False)]
print("There were",states_dup.shape[0],"duplicate rows")
states_dup

# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### checking for duplicate rows
###
zones_dup = zones.loc[zones.duplicated(keep=False)]
print("There were",zones_dup.shape[0],"duplicate rows")
zones_dup

# COMMAND ----------

# MAGIC %md
# MAGIC ##Product Gap Rules tab validations

# COMMAND ----------

# DBTITLE 1,Checking missed required fields
###
### checking for missed required fields
###
missed_gap_1 = prod_gap[(prod_gap.BU.isna()) | (prod_gap.Relationship_Type.isna()) | (prod_gap.Dep_Category.isna()) | (prod_gap.Dep_PF_Name.isna()) | (prod_gap.Ind_Category.isna()) | (prod_gap.Ind_PF_Name.isna()) | (prod_gap.Relationship_Rule.isna()) |((prod_gap.Relationship_Rule =="Other") & (prod_gap.Other_Rules.isna()))]

###
### orphan item with missed item's information
###
if set(BU).issubset(set(NA)):
  missed_gap_2 = prod_gap[(prod_gap.Dep_PF_Name == "No_Family") & ((prod_gap.Dep_Item_Name.isna()) | (prod_gap.Dep_UPC_Item_Number.isna()) | (prod_gap.Dep_Sell_Unit_Qty.isna()) | (prod_gap.Dep_Product_Key_Sys_ID_EAN_No_.isna()))]
  
  missed_gap_3 = prod_gap[(prod_gap.Ind_PF_Name == "No_Family") & ((prod_gap.Ind_Item_Name.isna()) | (prod_gap.Ind_UPC_Item_Number.isna()) |(prod_gap.Ind_Sell_Unit_Qty.isna()) | (prod_gap.Ind_Product_Key_Sys_ID_EAN_No_.isna()))]
else:
  missed_gap_2 = prod_gap[(prod_gap.Dep_PF_Name == "No_Family") & ((prod_gap.Dep_Item_Name.isna()) | (prod_gap.Dep_UPC_Item_Number.isna()) | (prod_gap.Dep_EAN_Flag.isna()) | (prod_gap.Dep_Product_Key_Sys_ID_EAN_No_.isna()))]

  missed_gap_3 = prod_gap[(prod_gap.Ind_PF_Name == "No_Family") & ((prod_gap.Ind_Item_Name.isna()) | (prod_gap.Ind_UPC_Item_Number.isna()) |(prod_gap.Ind_EAN_Flag.isna()) | (prod_gap.Ind_Product_Key_Sys_ID_EAN_No_.isna()))]
    

missed_gap = pd.concat([missed_gap_1,missed_gap_2,missed_gap_3]).drop_duplicates()
print("There were total",missed_gap_2.shape[0],"dependent items with missed cell")
print("There were total",missed_gap_3.shape[0],"independent items with missed cell")
print("There were total",missed_gap.shape[0],"rows with missed cell")
missed_gap

# COMMAND ----------

# DBTITLE 1,Checking duplicates
###
### checking for duplicate rows
###
prod_gap_dup = prod_gap.loc[prod_gap.duplicated(['BU', 'Relationship_Type', 'Dep_Category', 'Dep_PF_Name',
       'Dep_Item_Name', 'Dep_UPC_Item_Number', 'Dep_EAN_Flag',
       'Dep_Sell_Unit_Qty', 'Dep_Product_Key_Sys_ID_EAN_No_', 'Ind_Category',
       'Ind_PF_Name', 'Ind_Item_Name', 'Ind_UPC_Item_Number', 'Ind_EAN_Flag',
       'Ind_Sell_Unit_Qty', 'Ind_Product_Key_Sys_ID_EAN_No_'],keep=False)]
print("There were",prod_gap_dup.shape[0],"duplicate rows")
prod_gap_dup

# COMMAND ----------

# DBTITLE 1,Checking invalid inputs
###
### checking for invalid input
###

###
### Dep/Ind PF Name doesn't match relationship type
###
prod_gap_input_1 = prod_gap[((prod_gap.Relationship_Type == "PF to PF") & ((prod_gap.Dep_PF_Name =="No_Family")  | (prod_gap.Ind_PF_Name =="No_Family"))) | ((prod_gap.Relationship_Type == "PF to Item") & ((prod_gap.Ind_PF_Name !="No_Family") | (prod_gap.Dep_PF_Name =="No_Family"))) | ((prod_gap.Relationship_Type == "Item to PF") & ((prod_gap.Dep_PF_Name !="No_Family") | (prod_gap.Ind_PF_Name =="No_Family"))) | ((prod_gap.Relationship_Type == "Item to Item") & ((prod_gap.Dep_PF_Name !="No_Family") | (prod_gap.Ind_PF_Name !="No_Family")))]

###
### rule should be from standardize list
###
prod_gap_input_2 = prod_gap[~prod_gap.Relationship_Rule.isin(gap_rule_list)]

###
### relation type should be from standardize list
###
prod_gap_input_3 = prod_gap[~prod_gap.Relationship_Type.isin(gap_type_list)]

###
### inconsistant other ending number rule and comment 
###
prod_gap_input_4 = prod_gap[((prod_gap.Relationship_Rule=="Other") & (prod_gap.Other_Rules.isna())) | ((prod_gap.Relationship_Rule!="Other") & (prod_gap.Other_Rules.notna())) ]

prod_gap_input = pd.concat([prod_gap_input_1,prod_gap_input_2,prod_gap_input_3,prod_gap_input_4]).drop_duplicates()
print("There were",prod_gap_input_1.shape[0],"rows with inconsistant relationship type and PF name")
print("There were",prod_gap_input_2.shape[0],"rows with invalid product gap rule")
print("There were",prod_gap_input_3.shape[0],"rows with invalid relationship type")
print("There were",prod_gap_input_4.shape[0],"inconsistant Other relationship rule")
print("There were total",prod_gap_input.shape[0],"rows with invalid input")
prod_gap_input

# COMMAND ----------

# DBTITLE 1,Checking undefined combinations
###
### combinations of category , PF that are not defined in scope table
###
items_pf = items_scope[(items_scope.price_family != "No_Family")]
items_pf["concat"] = items_pf[["category_name","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
prod_gap_pf_dep = prod_gap[(prod_gap.Relationship_Type=="PF to Item" )| (prod_gap.Relationship_Type=="PF to PF") ]
prod_gap_pf_ind = prod_gap[(prod_gap.Relationship_Type=="Item to PF") | (prod_gap.Relationship_Type=="PF to PF")]
prod_gap_pf_dep['concat'] = prod_gap_pf_dep[["Dep_Category","Dep_PF_Name"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
prod_gap_pf_ind['concat'] = prod_gap_pf_ind[["Ind_Category","Ind_PF_Name"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
gap_pf_dep_undefined = prod_gap_pf_dep[~prod_gap_pf_dep.concat.isin(set(items_pf.concat))].drop_duplicates()
gap_pf_ind_undefined = prod_gap_pf_ind[~prod_gap_pf_ind.concat.isin(set(items_pf.concat))].drop_duplicates()

###
### combinations of category , PF, item_name, UPC, sell_unit_qty, TPK that are not defined in scope table
###
if set(BU).issubset(set(NA)):
  items_item = items_scope[(items_scope.price_family == "No_Family")]
  items_item["concat_1"] = items_item[["category_name","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  items_item["concat"] = items_item[["concat_1","item_name","upc","sell_unit_qty","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)

  prod_gap_item_ind = prod_gap[(prod_gap.Relationship_Type=="PF to Item" )| (prod_gap.Relationship_Type=="Item to Item") ]
  prod_gap_item_dep = prod_gap[(prod_gap.Relationship_Type=="Item to PF") | (prod_gap.Relationship_Type=="Item to Item")]
  prod_gap_item_dep['concat'] = prod_gap_item_dep[["Dep_Category","Dep_PF_Name",'Dep_Item_Name', 'Dep_UPC_Item_Number','Dep_Sell_Unit_Qty',"Dep_Product_Key_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  prod_gap_item_ind['concat'] = prod_gap_item_ind[["Ind_Category","Ind_PF_Name",'Ind_Item_Name', 'Ind_UPC_Item_Number','Ind_Sell_Unit_Qty', 'Ind_Product_Key_Sys_ID_EAN_No_' ]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  gap_item_dep_undefined = prod_gap_item_dep[~prod_gap_item_dep.concat.isin(set(items_item.concat))].drop_duplicates()
  gap_item_ind_undefined = prod_gap_item_ind[~prod_gap_item_ind.concat.isin(set(items_item.concat))].drop_duplicates()
else:
  
  items_item = items_scope[(items_scope.price_family == "No_Family")]
  items_item["concat_1"] =items_item[["category_name","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  items_item["concat"] = np.where((items_item.modelling_level
  =="EAN"), items_item[["concat_1","item_name","item_number","upc"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1),items_item[["concat_1","item_name","item_number","product_key"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1))
  
  prod_gap_item_ind = prod_gap[(prod_gap.Relationship_Type=="PF to Item" )| (prod_gap.Relationship_Type=="Item to Item") ]
  prod_gap_item_dep = prod_gap[(prod_gap.Relationship_Type=="Item to PF") | (prod_gap.Relationship_Type=="Item to Item")]
  prod_gap_item_dep['concat'] = prod_gap_item_dep[["Dep_Category","Dep_PF_Name",'Dep_Item_Name', 'Dep_UPC_Item_Number',"Dep_Product_Key_Sys_ID_EAN_No_"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  prod_gap_item_ind['concat'] = prod_gap_item_ind[["Ind_Category","Ind_PF_Name",'Ind_Item_Name', 'Ind_UPC_Item_Number', 'Ind_Product_Key_Sys_ID_EAN_No_' ]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)
  gap_item_dep_undefined = prod_gap_item_dep[~prod_gap_item_dep.concat.isin(set(items_item.concat))].drop_duplicates()
  gap_item_ind_undefined = prod_gap_item_ind[~prod_gap_item_ind.concat.isin(set(items_item.concat))].drop_duplicates()


pf_undifined = pd.concat([gap_pf_dep_undefined,gap_pf_ind_undefined,gap_item_dep_undefined,gap_item_ind_undefined]).drop(columns=["concat"]).drop_duplicates()
print("There were",gap_pf_dep_undefined.shape[0],"dependent PF that are not defined in scope table")
print("There were",gap_pf_ind_undefined.shape[0],"independent PF that are not defined in scope table")
print("There were",gap_item_dep_undefined.shape[0],"dependent item that are not defined in scope table")
print("There were",gap_item_ind_undefined.shape[0],"independent item that are not defined in scope table")
print("There were total",pf_undifined.shape[0],"combinations that are not defined in scope table")
pf_undifined

# COMMAND ----------

### sanity check for PF average
# retail_cat = retail_price[((retail_price.price_family != "No_Family") & retail_price.New_price_family.isna())| ((retail_price.New_price_family != "No_Family")& (retail_price.New_price_family.notna()))]
# retail_cat["concat"] = np.where((retail_cat.New_CSC.isna()), np.where((retail_cat.New_price_family.isna()),retail_cat[["category","sub_category","Category_Special_Classification","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1),retail_cat[["category","sub_category","Category_Special_Classification","New_price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)), np.where((retail_cat.New_price_family.isna()), retail_cat[["category","sub_category","New_CSC","price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1), retail_cat[["category","sub_category","New_CSC","New_price_family"]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)))
# retail_cat['concat_avg'] = retail_cat['avg'].groupby(retail_cat['concat']).transform('mean')
# from pyspark.sql import SparkSession
# #Create PySpark SparkSession
# pd.DataFrame(retail_cat)

# spark = SparkSession.builder \
#     .master("local[1]") \
#     .appName("SparkByExamples.com") \
#     .getOrCreate()
# #Create PySpark DataFrame from Pandas
# sparkDF=spark.createDataFrame(pd.DataFrame(retail_cat.drop(columns=["New_upc"])))
# display(sparkDF)

# COMMAND ----------

# Aadarsh 

# Check if the product gap rules are standard - rule language should be same as the Kartik's standard list of rules - values can be different (done MI)
# 

# COMMAND ----------

#Taru

# validations for the prod gap rules (unsure if its already there)
# Use v4 for the template to create these validations
#1- Make sure the dep/indep price families are correct and in scope. Highlight otherwise (since this sheet will be populated by the BU they may get the pf names etc wrong) (done MI)
#2- For any item level rule, validate the values in the columns specific to the item level info (i.e. UPC, Sell unit qty and product key should align with the item level inputs sheet) (done MI)
#3- check if the relationship/rule is one of the standard rules encoutered till now. Else flag for clarification from the BU. Get list of standardized rules from Taru/Karthik(done MI)

# COMMAND ----------

#taru

# CSC level bounds
# - flag cases where %the value is smaller than 1% in absolute value (done MI)

#Prod Gap Rules
# - dep/indp PF or item should have the category name as in item level inputs. Else flag (done MI)
